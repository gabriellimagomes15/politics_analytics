{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# formata-texto\n",
    "Esse script recebe arquivos .txt extraídos do site da Câmara dos Deputados e executa operações de limpeza e formatação usando regex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importação de pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIÁVEL GLOBAL PARA TESTAR VÁRIOS ENCODINGS\n",
    "encoding = 'UTF-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ler arquivos externos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fpaths(dir_path, pattern):\n",
    "    '''\n",
    "    >> DESCRIÇÃO\n",
    "    \n",
    "    Usa o módulo glob para buscar todos os arquivos\n",
    "    que correspondam ao padrão passado na variável \n",
    "    pattern'. Retorna uma lista de paths no formato \n",
    "    string. \n",
    "    \n",
    "    >> PARÂMETROS\n",
    "    \n",
    "    dir_path -> uma string com o caminho para o\n",
    "    diretório onde a busca pelos arquivos será\n",
    "    realizada.\n",
    "    \n",
    "    pattern -> uma string com o padrão de texto\n",
    "    que deve ser procurado no diretório.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    full_pattern = dir_path + pattern\n",
    "    files = glob.glob(full_pattern)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_list, zip_file = False):\n",
    "    random.seed(2020)\n",
    "    '''\n",
    "    >> DESCRIÇÃO\n",
    "    \n",
    "    Lê a lista de arquivos e configura o conteúdo\n",
    "    em um dataframe com os seguintes campos:\n",
    "    PRESIDENTE | CONTEUDO | ARQUIVO | ANO\n",
    "    Funciona para os discursos em plenário\n",
    "    \n",
    "    >> PARÂMETROS\n",
    "    \n",
    "    file_list -> uma lista de filepaths em formato\n",
    "    string. Ela é gerada anteriormente, na função\n",
    "    find_fpaths.\n",
    "    \n",
    "    '''\n",
    "    print('Reading Files...')\n",
    "    content = []\n",
    "    name_file_list = []\n",
    "    if zip_file:\n",
    "        for f in file_list:#[:5]:\n",
    "            #print('-->',f)\n",
    "            with zipfile.ZipFile(f) as zip_file:\n",
    "                print(f, len(zip_file.filelist))\n",
    "                ## serão selecionados 30% do total de cada arquivo de forma aleatória\n",
    "                sample_size = round(len(zip_file.filelist)*.3)\n",
    "                for name_file in random.sample(zip_file.filelist,sample_size):\n",
    "                    name_file_list.append(unidecode(name_file.filename))\n",
    "                    \n",
    "                    with io.TextIOWrapper(zip_file.open(name_file.filename), encoding=encoding) as arq:\n",
    "                        content.append(arq.read())\n",
    "\n",
    "    else:\n",
    "        # Lê o conteúdo dos arquivos de texto na lista\n",
    "        content = [ open(file, encoding = encoding).read() for file in file_list ]\n",
    "        name_file_list = [ unidecode(file) for file in file_list ]\n",
    "\n",
    "    return content, name_file_list\n",
    "\n",
    "def make_df_plen(file_list, content):\n",
    "    print('Making  DF plen...')\n",
    "    session_date = []\n",
    "    for file in info:\n",
    "        result = re.search(\"\\-([A-Z\\s]+)\\-\", file)# info[108251])\n",
    "        if result is None:\n",
    "            session_date.append('none')\n",
    "        else:\n",
    "            session_date.append(result.group(1))\n",
    "        \n",
    "    # Transforma eum dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'FILE'             : [ file for file in content ],\n",
    "        'ORIGINAL_CONTENT' : [ item for item in file_list ],\n",
    "        'CLEAN_CONTENT'    : [ unidecode(item) for item in file_list ],\n",
    "        'SESSION_TYPE'     : session_date,#[ re.search(\"\\-([A-Z\\s]+)\\-\", file).group(1) for file in content ],\n",
    "        'SESSION_DATE'     : [ re.search(\"\\d{8}\", file).group(0) for file in content ]\n",
    "    })\n",
    "    \n",
    "    df[\"SESSION_DATE\"] = pd.to_datetime(df.SESSION_DATE, format = \"%d%m%Y\")\n",
    "    df[\"MONTH\"] = df.SESSION_DATE.dt.month\n",
    "    df[\"YEAR\"] = df.SESSION_DATE.dt.year\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções de formatação e busca usando regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_speakers(string):\n",
    "    \n",
    "    '''\n",
    "    Essa função detecta o padrão de texto\n",
    "    que antecede a fala de um deputado e\n",
    "    retorna um objeto match (via re.find_all).\n",
    "    Ele é útil para detectar QUANTOS deputados\n",
    "    falaram em determinada string textual.\n",
    "    '''\n",
    "    \n",
    "    pattern = \"((O?\\s?SR\\.?\\s+?)|(A?\\s?SRA\\.?\\s+?))(\\s+DEPUTADO|\\s+DEPUTADA|\\s+PRESIDENTE|\\s+PRESIDENTA)?\"\n",
    "    regexp = re.compile(pattern)\n",
    "    matches = re.findall(regexp, string)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_full_quote(clean_string, original_string):\n",
    "    \n",
    "    '''\n",
    "    Essa função extrai todas as falas de Jair Bolsonaro\n",
    "    em uma determinada string. O pattern de regex encontra,\n",
    "    primeiro, uma fala qualquer do Presidente. Então, pega\n",
    "    tudo que está entre essa fala e a fala de outro parlamentar \n",
    "    ou o fim do arquivo. Isso é necessário porque há arquivos\n",
    "    que misturam a fala de vários parlamentares, geralmente\n",
    "    quando estão envolvidos em uma discussão.\n",
    "    \n",
    "    Para fazer essa operação, são passados textos sem caracteres\n",
    "    especiais unicode. Depois de feita a captura, usamos os índices\n",
    "    das matches no regex para extrair o mesmo pedaço de texto\n",
    "    na string original.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #if \"O SR. PRESIDENTE (Jair Bolsonaro)\" in clean_string:\n",
    "     #   pattern = \"O SR\\. PRESIDENTE (\\(Jair Bolsonaro\\))?(.*?)((O?\\s?SR\\.?\\s+?)|(A?\\s?SRA\\.?\\s+?)|$)\"\n",
    "      #  group_no = 2\n",
    "\n",
    "    #else:\n",
    "    #pattern  = \"O?\\s?SR\\.?\\s?(DEPUTADO)?\\s+JAIR\\s+BOLSONARO\\s?(\\((Bloco\\/)?\\w{2,}\\s?\\-\\s?\\w{2}[^)]+\\))?(.*?)((O?\\s?SR\\.?\\s+?)|(A?\\s?SRA\\.?\\s+?)|$)\"\n",
    "    pattern = \"((O?\\s?SR\\.?\\s+?)|(A?\\s?SRA\\.?\\s+?))(\\s+DEPUTADO|\\s+DEPUTADA|\\s+PRESIDENTE|\\s+PRESIDENTA)?\"\n",
    "    group_no = 4\n",
    "        \n",
    "    regexp   = re.compile(pattern, re.MULTILINE)\n",
    "    matches  = re.finditer(regexp, clean_string)\n",
    "    \n",
    "    full_clean_quote    = [ ]\n",
    "    full_original_quote = [ ]\n",
    "    \n",
    "    for match in matches:\n",
    "                        \n",
    "        match_start = match.start(group_no)\n",
    "        match_end   = match.end(group_no)\n",
    "            \n",
    "        clean_quote = match[group_no]\n",
    "        clean_quote = clean_quote.replace(\"- \", \"\")\n",
    "        \n",
    "        original_quote = original_string[match_start:match_end]\n",
    "        original_quote = original_quote.replace(\"- \", \"\")\n",
    "        \n",
    "        full_clean_quote.append(clean_quote)\n",
    "        full_original_quote.append(original_quote)\n",
    "        \n",
    "    full_clean_quote    = ' [ INTERRUPÇÃO ] '.join(full_clean_quote)\n",
    "    full_original_quote = ' [ INTERRUPÇÃO ] '.join(full_original_quote)\n",
    "\n",
    "    # Remove espaços múltiplos internos usando a operação join de lista\n",
    "    full_clean_quote    = ' '.join(full_clean_quote.split()) \n",
    "    full_original_quote = ' '.join(full_original_quote.split()) \n",
    "    \n",
    "    return full_clean_quote, full_original_quote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções para aplicar operações de regex no dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_count_speakers(row):\n",
    "    \n",
    "    '''\n",
    "    Aplica, linha a linha, a função\n",
    "    find_speakers(string)\n",
    "    '''\n",
    "    \n",
    "    matches = find_speakers(row.CLEAN_CONTENT)\n",
    "    speaker_count = len(matches)\n",
    "    \n",
    "    return pd.Series({ \"SPEAKER_COUNT\":speaker_count \n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_extract_full_quote(row):\n",
    "    \n",
    "    '''\n",
    "    Aplica, linha a linha, a função\n",
    "    extract_full_quote(string)\n",
    "    '''\n",
    "    \n",
    "    full_clean_quote, full_original_quote = extract_full_quote(row.CLEAN_CONTENT, row.ORIGINAL_CONTENT)\n",
    "    return pd.Series({\n",
    "        \"FULL_CLEAN_QUOTE\"    : full_clean_quote,\n",
    "        \"FULL_ORIGINAL_QUOTE\" : full_original_quote\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função que encapsula anteriores e roda a operação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speak(zip_file = False):\n",
    "    '''\n",
    "    zip_file -> indica se os arquivos estão compactados no formato ZIP.\n",
    "    \n",
    "    Executa as operações anteriores em ambos os bancos de dados\n",
    "    (plenário e comissões), filtra entradas sem match, concatena\n",
    "    ambos os dataframes e salva para arquivo csv.\n",
    "    '''\n",
    "    list_info_file = []\n",
    "    list_speak = []\n",
    "    \n",
    "    if zip_file:\n",
    "        #list_speak, list_info_file = read_file( find_fpaths(\"../data/txts/plenario/\", \"*.zip\"), zip_file)\n",
    "        list_speak, list_info_file = read_file( find_fpaths(\"/disco01/gabriel/plenario/\", \"*.zip\"), zip_file)\n",
    "    else:\n",
    "        list_speak, list_info_file = read_file(find_fpaths(\"../data/txts/plenario/\", \"*.txt\"),zip_file)\n",
    "        \n",
    "    \n",
    "    return list_speak, list_info_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(year = None):\n",
    "    if year:\n",
    "        metadata = pd.read_csv(path+'plenario/tables/{}-plenario-metadata.csv'.format(year))\n",
    "        #print(metadata.shape)\n",
    "        return metadata\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Files...\n",
      "/disco01/gabriel/plenario/2001.zip 19355\n",
      "/disco01/gabriel/plenario/2005.zip 20185\n",
      "/disco01/gabriel/plenario/2012.zip 20031\n",
      "/disco01/gabriel/plenario/2002.zip 10353\n",
      "/disco01/gabriel/plenario/2013.zip 27752\n",
      "/disco01/gabriel/plenario/2014.zip 17303\n",
      "/disco01/gabriel/plenario/2010.zip 14961\n",
      "/disco01/gabriel/plenario/2003.zip 23829\n",
      "/disco01/gabriel/plenario/2016.zip 23092\n",
      "/disco01/gabriel/plenario/2019.zip 19506\n",
      "/disco01/gabriel/plenario/2017.zip 28134\n",
      "/disco01/gabriel/plenario/2018.zip 16560\n",
      "/disco01/gabriel/plenario/2009.zip 24499\n",
      "/disco01/gabriel/plenario/2011.zip 22727\n",
      "/disco01/gabriel/plenario/2008.zip 20488\n",
      "/disco01/gabriel/plenario/2007.zip 25116\n",
      "/disco01/gabriel/plenario/2006.zip 15522\n",
      "/disco01/gabriel/plenario/2015.zip 28169\n",
      "/disco01/gabriel/plenario/2004.zip 19028\n"
     ]
    }
   ],
   "source": [
    "## LENDO OS DISCURSOS E SALVANDO EM UM DATAFRAME\n",
    "path = '/disco01/gabriel/'\n",
    "speak, info = get_speak(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "#speak[i],info[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making  DF plen...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4bee824579b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_plen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_df_plen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeak\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#df_plen['INFO'] = info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#As sessões do tipo HOMENAGEM são apenas registro de protocolo.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Não contém transcrição de discursos.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e36435fa6635>\u001b[0m in \u001b[0;36mmake_df_plen\u001b[0;34m(file_list, content)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m'CLEAN_CONTENT'\u001b[0m    \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0munidecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_list\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;34m'SESSION_TYPE'\u001b[0m     \u001b[0;34m:\u001b[0m \u001b[0msession_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m#[ re.search(\"\\-([A-Z\\s]+)\\-\", file).group(1) for file in content ],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;34m'SESSION_DATE'\u001b[0m     \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\d{8}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     })\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter_venv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    409\u001b[0m             )\n\u001b[1;32m    410\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter_venv/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         ]\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter_venv/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter_venv/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arrays must all be same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "df_plen = make_df_plen(speak[0:5],info[0:5])\n",
    "#df_plen['INFO'] = info\n",
    "\n",
    "#As sessões do tipo HOMENAGEM são apenas registro de protocolo. \n",
    "#Não contém transcrição de discursos.\n",
    "df_plen = df_plen[ df_plen.SESSION_TYPE != \"HOMENAGEM\" ]\n",
    "\n",
    "# Aplica funções para extrair discursos\n",
    "df_plen[\"SPEAKER_COUNT\"] = df_plen.apply(apply_count_speakers, axis=1)\n",
    "df_plen.index = [i for i in range(len(df_plen))]\n",
    "\n",
    "#return df_plen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_plen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inserindo metadados no data frame dos discursos\n",
    "df_metadata = pd.DataFrame()\n",
    "for y in df_plen.YEAR.unique():\n",
    "    print(y)\n",
    "    metadata = get_metadata(y)\n",
    "    if metadata:\n",
    "        for f in df_plen.loc[df_plen.YEAR == y,'FILE'].apply(lambda x: x.split('-')[0]):\n",
    "            f = int(f)\n",
    "            df_metadata = pd.concat([df_metadata,metadata.loc[f:f,['Orador','Hora']]])\n",
    "    \n",
    "df_metadata.index = [i for i in range(len(df_metadata))]\n",
    "\n",
    "df_plen = pd.concat([df_plen,df_metadata],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_plen.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orador = []\n",
    "partido = []\n",
    "ufPartido = []\n",
    "\n",
    "for i,d in enumerate(df_plen.Orador):#[0].split('-')\n",
    "    splitOrador  = d.split(',')\n",
    "    #print(i,d,splitOrador)\n",
    "    \n",
    "    if len(splitOrador) > 1:\n",
    "        splitPartido = splitOrador[1].split('-')\n",
    "        \n",
    "        if len(splitPartido) < 2:\n",
    "            partido.append('N/I')\n",
    "            ufPartido.append('N/I')\n",
    "        else:\n",
    "            partido.append(splitPartido[0])\n",
    "            ufPartido.append(splitPartido[1])\n",
    "    else:\n",
    "        splitPartido = splitOrador[0].split('-')\n",
    "        partido.append('N/I')\n",
    "        ufPartido.append('N/I')\n",
    "    \n",
    "    orador.append(splitOrador[0])\n",
    "    if len(splitPartido) < 2 or len(splitPartido) > 2:\n",
    "        print(d,splitPartido)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_plen['ORADOR_CLEAN'] = orador\n",
    "df_plen['PARTIDO']      = partido\n",
    "df_plen['UF_PARTIDO']   = ufPartido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plen.index = [i for i in range(len(df_plen))]\n",
    "len(df_plen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva\n",
    "directory = path #\"../data/csvs/\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "#name_file = directory + 'discursos_plen.csv'#str(df.YEAR.unique()[0]) + '_plen.csv'\n",
    "#df_plen.to_csv(name_file, index = False)\n",
    "\n",
    "name_file = directory + 'discursos_plen.json'#str(df.YEAR.unique()[0]) + '_plen.csv'\n",
    "df_plen.to_json(name_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_metadata\n",
    "d.split(',')\n",
    "pd.Series(ufPartido).value_counts()\n",
    "#metadata.loc[int(f):int(f),['Orador','Hora']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plen.head(10)\n",
    "#len(df_plen)\n",
    "#metadata.loc[f:int(f),['Orador','Hora']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plen.index = [i for i in range(len(df_plen))]\n",
    "df_plen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame([1,2,3])\n",
    "b = pd.DataFrame([10,11,12])\n",
    "\n",
    "pd.concat([a,b],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Qtd de Registros por ANO:\\n{}\\n\\nQtd Total de registros:{} '.format(df_plen.YEAR.value_counts(),len(df_plen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "session_date = []\n",
    "for file in info:\n",
    "    #print(i)\n",
    "    result = re.search(\"\\-([A-Z\\s]+)\\-\", file)# info[108251])\n",
    "    if result is None:\n",
    "        session_date.append('none')\n",
    "    else:\n",
    "        session_date.append(result.group(1))\n",
    "    #'SESSION_DATE'     : [ re.search(\"\\d{8}\", file).group(0) for file in content ]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(session_date)\n",
    "len(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
