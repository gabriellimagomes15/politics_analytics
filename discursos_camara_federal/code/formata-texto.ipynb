{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# formata-texto\n",
    "Esse script recebe arquivos .txt extraídos do site da Câmara dos Deputados e executa operações de limpeza e formatação usando regex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importação de pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIÁVEL GLOBAL PARA TESTAR VÁRIOS ENCODINGS\n",
    "encoding = 'UTF-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ler arquivos externos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fpaths(dir_path, pattern):\n",
    "    '''\n",
    "    >> DESCRIÇÃO\n",
    "    \n",
    "    Usa o módulo glob para buscar todos os arquivos\n",
    "    que correspondam ao padrão passado na variável \n",
    "    pattern'. Retorna uma lista de paths no formato \n",
    "    string. \n",
    "    \n",
    "    >> PARÂMETROS\n",
    "    \n",
    "    dir_path -> uma string com o caminho para o\n",
    "    diretório onde a busca pelos arquivos será\n",
    "    realizada.\n",
    "    \n",
    "    pattern -> uma string com o padrão de texto\n",
    "    que deve ser procurado no diretório.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    full_pattern = dir_path + pattern\n",
    "    files = glob.glob(full_pattern)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_list, zip_file = False):\n",
    "    '''\n",
    "    >> DESCRIÇÃO\n",
    "    \n",
    "    Lê a lista de arquivos e configura o conteúdo\n",
    "    em um dataframe com os seguintes campos:\n",
    "    PRESIDENTE | CONTEUDO | ARQUIVO | ANO\n",
    "    Funciona para os discursos em plenário\n",
    "    \n",
    "    >> PARÂMETROS\n",
    "    \n",
    "    file_list -> uma lista de filepaths em formato\n",
    "    string. Ela é gerada anteriormente, na função\n",
    "    find_fpaths.\n",
    "    \n",
    "    '''\n",
    "    print('Reading Files...')\n",
    "    content = []\n",
    "    name_file_list = []\n",
    "    if zip_file:\n",
    "        for f in file_list:#[:5]:\n",
    "            with zipfile.ZipFile(f) as zip_file:\n",
    "                print(f, len(zip_file.filelist))\n",
    "                ## serão selecionados apenas 5% do total de cada arquivo\n",
    "                sample_size = round(len(zip_file.filelist)*.3)\n",
    "                for name_file in random.sample(zip_file.filelist,sample_size):\n",
    "                    name_file_list.append(unidecode(name_file.filename))\n",
    "                    with io.TextIOWrapper(zip_file.open(name_file.filename), encoding=\"utf-8\") as arq:\n",
    "                        content.append(arq.read())\n",
    "\n",
    "    else:\n",
    "        # Lê o conteúdo dos arquivos de texto na lista\n",
    "        content = [ open(file, encoding = encoding).read() for file in file_list ]\n",
    "        name_file_list = [ unidecode(file) for file in file_list ]\n",
    "\n",
    "    return content, name_file_list\n",
    "\n",
    "def make_df_plen(file_list, content):\n",
    "    print('Making  DF plen...')\n",
    "    \n",
    "    # Transforma eum dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'FILE'             : [ file for file in file_list ],\n",
    "        'ORIGINAL_CONTENT' : [ item for item in content ],\n",
    "        'CLEAN_CONTENT'    : [ unidecode(item) for item in content ],\n",
    "        'SESSION_TYPE'     : [ re.search(\"\\-([A-Z\\s]+)\\-\", file).group(1) for file in file_list ],\n",
    "        'SESSION_DATE'     : [ re.search(\"\\d{8}\", file).group(0) for file in file_list ]\n",
    "    })\n",
    "    \n",
    "    df[\"SESSION_DATE\"] = pd.to_datetime(df.SESSION_DATE, format = \"%d%m%Y\")\n",
    "    df[\"MONTH\"] = df.SESSION_DATE.dt.month\n",
    "    df[\"YEAR\"] = df.SESSION_DATE.dt.year\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções de formatação e busca usando regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_speakers(string):\n",
    "    \n",
    "    '''\n",
    "    Essa função detecta o padrão de texto\n",
    "    que antecede a fala de um deputado e\n",
    "    retorna um objeto match (via re.find_all).\n",
    "    Ele é útil para detectar QUANTOS deputados\n",
    "    falaram em determinada string textual.\n",
    "    '''\n",
    "    \n",
    "    pattern = \"((O?\\s?SR\\.?\\s+?)|(A?\\s?SRA\\.?\\s+?))(\\s+DEPUTADO|\\s+DEPUTADA|\\s+PRESIDENTE|\\s+PRESIDENTA)?\"\n",
    "    regexp = re.compile(pattern)\n",
    "    matches = re.findall(regexp, string)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_full_quote(clean_string, original_string):\n",
    "    \n",
    "    '''\n",
    "    Essa função extrai todas as falas de Jair Bolsonaro\n",
    "    em uma determinada string. O pattern de regex encontra,\n",
    "    primeiro, uma fala qualquer do Presidente. Então, pega\n",
    "    tudo que está entre essa fala e a fala de outro parlamentar \n",
    "    ou o fim do arquivo. Isso é necessário porque há arquivos\n",
    "    que misturam a fala de vários parlamentares, geralmente\n",
    "    quando estão envolvidos em uma discussão.\n",
    "    \n",
    "    Para fazer essa operação, são passados textos sem caracteres\n",
    "    especiais unicode. Depois de feita a captura, usamos os índices\n",
    "    das matches no regex para extrair o mesmo pedaço de texto\n",
    "    na string original.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #if \"O SR. PRESIDENTE (Jair Bolsonaro)\" in clean_string:\n",
    "     #   pattern = \"O SR\\. PRESIDENTE (\\(Jair Bolsonaro\\))?(.*?)((O?\\s?SR\\.?\\s+?)|(A?\\s?SRA\\.?\\s+?)|$)\"\n",
    "      #  group_no = 2\n",
    "\n",
    "    #else:\n",
    "    #pattern  = \"O?\\s?SR\\.?\\s?(DEPUTADO)?\\s+JAIR\\s+BOLSONARO\\s?(\\((Bloco\\/)?\\w{2,}\\s?\\-\\s?\\w{2}[^)]+\\))?(.*?)((O?\\s?SR\\.?\\s+?)|(A?\\s?SRA\\.?\\s+?)|$)\"\n",
    "    pattern = \"((O?\\s?SR\\.?\\s+?)|(A?\\s?SRA\\.?\\s+?))(\\s+DEPUTADO|\\s+DEPUTADA|\\s+PRESIDENTE|\\s+PRESIDENTA)?\"\n",
    "    group_no = 4\n",
    "        \n",
    "    regexp   = re.compile(pattern, re.MULTILINE)\n",
    "    matches  = re.finditer(regexp, clean_string)\n",
    "    \n",
    "    full_clean_quote    = [ ]\n",
    "    full_original_quote = [ ]\n",
    "    \n",
    "    for match in matches:\n",
    "                        \n",
    "        match_start = match.start(group_no)\n",
    "        match_end   = match.end(group_no)\n",
    "            \n",
    "        clean_quote = match[group_no]\n",
    "        clean_quote = clean_quote.replace(\"- \", \"\")\n",
    "        \n",
    "        original_quote = original_string[match_start:match_end]\n",
    "        original_quote = original_quote.replace(\"- \", \"\")\n",
    "        \n",
    "        full_clean_quote.append(clean_quote)\n",
    "        full_original_quote.append(original_quote)\n",
    "        \n",
    "    full_clean_quote    = ' [ INTERRUPÇÃO ] '.join(full_clean_quote)\n",
    "    full_original_quote = ' [ INTERRUPÇÃO ] '.join(full_original_quote)\n",
    "\n",
    "    # Remove espaços múltiplos internos usando a operação join de lista\n",
    "    full_clean_quote    = ' '.join(full_clean_quote.split()) \n",
    "    full_original_quote = ' '.join(full_original_quote.split()) \n",
    "    \n",
    "    return full_clean_quote, full_original_quote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções para aplicar operações de regex no dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_count_speakers(row):\n",
    "    \n",
    "    '''\n",
    "    Aplica, linha a linha, a função\n",
    "    find_speakers(string)\n",
    "    '''\n",
    "    \n",
    "    matches = find_speakers(row.CLEAN_CONTENT)\n",
    "    speaker_count = len(matches)\n",
    "    \n",
    "    return pd.Series({ \"SPEAKER_COUNT\":speaker_count \n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_extract_full_quote(row):\n",
    "    \n",
    "    '''\n",
    "    Aplica, linha a linha, a função\n",
    "    extract_full_quote(string)\n",
    "    '''\n",
    "    \n",
    "    full_clean_quote, full_original_quote = extract_full_quote(row.CLEAN_CONTENT, row.ORIGINAL_CONTENT)\n",
    "    return pd.Series({\n",
    "        \"FULL_CLEAN_QUOTE\"    : full_clean_quote,\n",
    "        \"FULL_ORIGINAL_QUOTE\" : full_original_quote\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função que encapsula anteriores e roda a operação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_script(zip_file = False):\n",
    "    '''\n",
    "    zip_file -> indica se os arquivos estão compactados no formato ZIP.\n",
    "    \n",
    "    Executa as operações anteriores em ambos os bancos de dados\n",
    "    (plenário e comissões), filtra entradas sem match, concatena\n",
    "    ambos os dataframes e salva para arquivo csv.\n",
    "    '''\n",
    "    list_info_file = []\n",
    "    list_speak = []\n",
    "    \n",
    "    if zip_file:\n",
    "        list_speak, list_info_file = read_file( find_fpaths(\"../data/txts/plenario/\", \"*.zip\"), zip_file)\n",
    "    else:\n",
    "        list_speak, list_info_file = read_file(find_fpaths(\"../data/txts/plenario/\", \"*.txt\"),zip_file)\n",
    "        \n",
    "    df_plen = make_df_plen(list_info_file,list_speak)\n",
    "    \n",
    "    #As sessões do tipo HOMENAGEM são apenas registro de protocolo. \n",
    "    #Não contém transcrição de discursos.\n",
    "    df_plen = df_plen[ df_plen.SESSION_TYPE != \"HOMENAGEM\" ]\n",
    "    \n",
    "    # Aplica funções para extrair discursos\n",
    "    df_plen[\"SPEAKER_COUNT\"] = df_plen.apply(apply_count_speakers, axis=1)\n",
    "    \n",
    "    return df_plen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILE</th>\n",
       "      <th>ORIGINAL_CONTENT</th>\n",
       "      <th>CLEAN_CONTENT</th>\n",
       "      <th>SESSION_TYPE</th>\n",
       "      <th>SESSION_DATE</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>SPEAKER_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11548-29052001-17h00-ORDEM DO DIA-094-3-51-O.txt</td>\n",
       "      <td>O SR. EURÍPEDES MIRANDA (Bloco/PDT-RO. Pela or...</td>\n",
       "      <td>O SR. EURIPEDES MIRANDA (Bloco/PDT-RO. Pela or...</td>\n",
       "      <td>ORDEM DO DIA</td>\n",
       "      <td>2001-05-29</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>15346-28032001-17h08-ORDEM DO DIA-033-3-51-O.txt</td>\n",
       "      <td>O SR. IÉDIO ROSA (Bloco/PSB-RJ. Pela ordem. Se...</td>\n",
       "      <td>O SR. IEDIO ROSA (Bloco/PSB-RJ. Pela ordem. Se...</td>\n",
       "      <td>ORDEM DO DIA</td>\n",
       "      <td>2001-03-28</td>\n",
       "      <td>3</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>25-27122001-22h46-ORDEM DO DIA-004-5-51-C.txt</td>\n",
       "      <td>O SR. PAUDERNEY AVELINO (Bloco/PFL-AM. Sem rev...</td>\n",
       "      <td>O SR. PAUDERNEY AVELINO (Bloco/PFL-AM. Sem rev...</td>\n",
       "      <td>ORDEM DO DIA</td>\n",
       "      <td>2001-12-27</td>\n",
       "      <td>12</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>12263-17052001-11h28-ORDEM DO DIA-083-3-51-O.txt</td>\n",
       "      <td>O SR. ALBERTO FRAGA (PMDB-DF. Pela ordem. Sem ...</td>\n",
       "      <td>O SR. ALBERTO FRAGA (PMDB-DF. Pela ordem. Sem ...</td>\n",
       "      <td>ORDEM DO DIA</td>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8764-15082001-18h02-COMUNICACOES PARLAMENTARES...</td>\n",
       "      <td>O SR. IVAN VALENTE (PT-SP. Pela ordem. Pronunc...</td>\n",
       "      <td>O SR. IVAN VALENTE (PT-SP. Pela ordem. Pronunc...</td>\n",
       "      <td>COMUNICACOES PARLAMENTARES</td>\n",
       "      <td>2001-08-15</td>\n",
       "      <td>8</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                FILE  \\\n",
       "0   11548-29052001-17h00-ORDEM DO DIA-094-3-51-O.txt   \n",
       "1   15346-28032001-17h08-ORDEM DO DIA-033-3-51-O.txt   \n",
       "2      25-27122001-22h46-ORDEM DO DIA-004-5-51-C.txt   \n",
       "3   12263-17052001-11h28-ORDEM DO DIA-083-3-51-O.txt   \n",
       "5  8764-15082001-18h02-COMUNICACOES PARLAMENTARES...   \n",
       "\n",
       "                                    ORIGINAL_CONTENT  \\\n",
       "0  O SR. EURÍPEDES MIRANDA (Bloco/PDT-RO. Pela or...   \n",
       "1  O SR. IÉDIO ROSA (Bloco/PSB-RJ. Pela ordem. Se...   \n",
       "2  O SR. PAUDERNEY AVELINO (Bloco/PFL-AM. Sem rev...   \n",
       "3  O SR. ALBERTO FRAGA (PMDB-DF. Pela ordem. Sem ...   \n",
       "5  O SR. IVAN VALENTE (PT-SP. Pela ordem. Pronunc...   \n",
       "\n",
       "                                       CLEAN_CONTENT  \\\n",
       "0  O SR. EURIPEDES MIRANDA (Bloco/PDT-RO. Pela or...   \n",
       "1  O SR. IEDIO ROSA (Bloco/PSB-RJ. Pela ordem. Se...   \n",
       "2  O SR. PAUDERNEY AVELINO (Bloco/PFL-AM. Sem rev...   \n",
       "3  O SR. ALBERTO FRAGA (PMDB-DF. Pela ordem. Sem ...   \n",
       "5  O SR. IVAN VALENTE (PT-SP. Pela ordem. Pronunc...   \n",
       "\n",
       "                 SESSION_TYPE SESSION_DATE  MONTH  YEAR  SPEAKER_COUNT  \n",
       "0                ORDEM DO DIA   2001-05-29      5  2001              1  \n",
       "1                ORDEM DO DIA   2001-03-28      3  2001              1  \n",
       "2                ORDEM DO DIA   2001-12-27     12  2001              1  \n",
       "3                ORDEM DO DIA   2001-05-17      5  2001              1  \n",
       "5  COMUNICACOES PARLAMENTARES   2001-08-15      8  2001              1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Files...\n",
      "../data/txts/plenario\\2001.zip 19355\n",
      "../data/txts/plenario\\2002.zip 10353\n",
      "../data/txts/plenario\\2003.zip 23829\n",
      "../data/txts/plenario\\2004.zip 19028\n",
      "../data/txts/plenario\\2005.zip 20185\n",
      "../data/txts/plenario\\2006.zip 15522\n",
      "../data/txts/plenario\\2007.zip 25116\n",
      "../data/txts/plenario\\2008.zip 20488\n",
      "../data/txts/plenario\\2009.zip 24499\n",
      "../data/txts/plenario\\2010.zip 14961\n",
      "../data/txts/plenario\\2011.zip 22727\n",
      "../data/txts/plenario\\2012.zip 20031\n",
      "../data/txts/plenario\\2013.zip 27752\n",
      "../data/txts/plenario\\2014.zip 17303\n",
      "../data/txts/plenario\\2015.zip 28169\n",
      "../data/txts/plenario\\2016.zip 23092\n",
      "../data/txts/plenario\\2017.zip 28134\n",
      "../data/txts/plenario\\2018.zip 16560\n",
      "../data/txts/plenario\\2019.zip 19506\n",
      "Making  DF plen...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-6f41678a880e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## LENDO OS DISCURSOS E SALVANDO EM UM DATAFRAME\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-c5b1526d9c89>\u001b[0m in \u001b[0;36mrun_script\u001b[1;34m(zip_file)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mlist_speak\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_info_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind_fpaths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../data/txts/plenario/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"*.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mzip_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mdf_plen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_df_plen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_info_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist_speak\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m#As sessões do tipo HOMENAGEM são apenas registro de protocolo.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-b885c0fa4d6e>\u001b[0m in \u001b[0;36mmake_df_plen\u001b[1;34m(file_list, content)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;34m'ORIGINAL_CONTENT'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;34m'CLEAN_CONTENT'\u001b[0m    \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0munidecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;34m'SESSION_TYPE'\u001b[0m     \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\-([A-Z\\s]+)\\-\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_list\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;34m'SESSION_DATE'\u001b[0m     \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\d{8}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_list\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     })\n",
      "\u001b[1;32m<ipython-input-20-b885c0fa4d6e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;34m'ORIGINAL_CONTENT'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;34m'CLEAN_CONTENT'\u001b[0m    \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0munidecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;34m'SESSION_TYPE'\u001b[0m     \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\-([A-Z\\s]+)\\-\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_list\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;34m'SESSION_DATE'\u001b[0m     \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\d{8}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_list\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     })\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "## LENDO OS DISCURSOS E SALVANDO EM UM DATAFRAME\n",
    "df = run_script(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Qtd de Registros por ANO:\\n{}\\n\\nQtd Total de registros:{} '.format(df.YEAR.value_counts(),len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva\n",
    "directory = \"../data/csvs/\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "name_file = directory + 'discursos_plen.csv'#str(df.YEAR.unique()[0]) + '_plen.csv'\n",
    "df.to_csv(name_file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
