{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# formata-texto\n",
    "Esse script recebe arquivos .txt extraídos do site da Câmara dos Deputados e executa operações de limpeza e formatação usando regex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importação de pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIÁVEL GLOBAL PARA TESTAR VÁRIOS ENCODINGS\n",
    "encoding = 'UTF-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ler arquivos externos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fpaths(dir_path, pattern):\n",
    "    '''\n",
    "    >> DESCRIÇÃO\n",
    "    \n",
    "    Usa o módulo glob para buscar todos os arquivos\n",
    "    que correspondam ao padrão passado na variável \n",
    "    pattern'. Retorna uma lista de paths no formato \n",
    "    string. \n",
    "    \n",
    "    >> PARÂMETROS\n",
    "    \n",
    "    dir_path -> uma string com o caminho para o\n",
    "    diretório onde a busca pelos arquivos será\n",
    "    realizada.\n",
    "    \n",
    "    pattern -> uma string com o padrão de texto\n",
    "    que deve ser procurado no diretório.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    full_pattern = dir_path + pattern\n",
    "    files = glob.glob(full_pattern)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_list, zip_file = False):\n",
    "    '''\n",
    "    >> DESCRIÇÃO\n",
    "    \n",
    "    Lê a lista de arquivos e configura o conteúdo\n",
    "    em um dataframe com os seguintes campos:\n",
    "    PRESIDENTE | CONTEUDO | ARQUIVO | ANO\n",
    "    Funciona para os discursos em plenário\n",
    "    \n",
    "    >> PARÂMETROS\n",
    "    \n",
    "    file_list -> uma lista de filepaths em formato\n",
    "    string. Ela é gerada anteriormente, na função\n",
    "    find_fpaths.\n",
    "    \n",
    "    '''\n",
    "    print('Reading Files...')\n",
    "    content = []\n",
    "    name_file_list = []\n",
    "    if zip_file:\n",
    "        for f in file_list:#[:5]:\n",
    "            with zipfile.ZipFile(f) as zip_file:\n",
    "                print(f, len(zip_file.filelist))\n",
    "                ## serão selecionados 30% do total de cada arquivo de forma aleatória\n",
    "                sample_size = round(len(zip_file.filelist)*.3)\n",
    "                for name_file in random.sample(zip_file.filelist,sample_size):\n",
    "                    name_file_list.append(unidecode(name_file.filename))\n",
    "                    \n",
    "                    with io.TextIOWrapper(zip_file.open(name_file.filename), encoding=encoding) as arq:\n",
    "                        content.append(arq.read())\n",
    "\n",
    "    else:\n",
    "        # Lê o conteúdo dos arquivos de texto na lista\n",
    "        content = [ open(file, encoding = encoding).read() for file in file_list ]\n",
    "        name_file_list = [ unidecode(file) for file in file_list ]\n",
    "\n",
    "    return content, name_file_list\n",
    "\n",
    "def make_df_plen(file_list, content):\n",
    "    print('Making  DF plen...')\n",
    "    session_date = []\n",
    "    for file in info:\n",
    "        result = re.search(\"\\-([A-Z\\s]+)\\-\", file)# info[108251])\n",
    "        if result is None:\n",
    "            session_date.append('none')\n",
    "        else:\n",
    "            session_date.append(result.group(1))\n",
    "        \n",
    "    # Transforma eum dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'FILE'             : [ file for file in file_list ],\n",
    "        'ORIGINAL_CONTENT' : [ item for item in file_list ],\n",
    "        'CLEAN_CONTENT'    : [ unidecode(item) for item in file_list ],\n",
    "        'SESSION_TYPE'     : session_date,#[ re.search(\"\\-([A-Z\\s]+)\\-\", file).group(1) for file in content ],\n",
    "        'SESSION_DATE'     : [ re.search(\"\\d{8}\", file).group(0) for file in content ]\n",
    "    })\n",
    "    \n",
    "    df[\"SESSION_DATE\"] = pd.to_datetime(df.SESSION_DATE, format = \"%d%m%Y\")\n",
    "    df[\"MONTH\"] = df.SESSION_DATE.dt.month\n",
    "    df[\"YEAR\"] = df.SESSION_DATE.dt.year\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções de formatação e busca usando regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_speakers(string):\n",
    "    \n",
    "    '''\n",
    "    Essa função detecta o padrão de texto\n",
    "    que antecede a fala de um deputado e\n",
    "    retorna um objeto match (via re.find_all).\n",
    "    Ele é útil para detectar QUANTOS deputados\n",
    "    falaram em determinada string textual.\n",
    "    '''\n",
    "    \n",
    "    pattern = \"((O?\\s?SR\\.?\\s+?)|(A?\\s?SRA\\.?\\s+?))(\\s+DEPUTADO|\\s+DEPUTADA|\\s+PRESIDENTE|\\s+PRESIDENTA)?\"\n",
    "    regexp = re.compile(pattern)\n",
    "    matches = re.findall(regexp, string)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_full_quote(clean_string, original_string):\n",
    "    \n",
    "    '''\n",
    "    Essa função extrai todas as falas de Jair Bolsonaro\n",
    "    em uma determinada string. O pattern de regex encontra,\n",
    "    primeiro, uma fala qualquer do Presidente. Então, pega\n",
    "    tudo que está entre essa fala e a fala de outro parlamentar \n",
    "    ou o fim do arquivo. Isso é necessário porque há arquivos\n",
    "    que misturam a fala de vários parlamentares, geralmente\n",
    "    quando estão envolvidos em uma discussão.\n",
    "    \n",
    "    Para fazer essa operação, são passados textos sem caracteres\n",
    "    especiais unicode. Depois de feita a captura, usamos os índices\n",
    "    das matches no regex para extrair o mesmo pedaço de texto\n",
    "    na string original.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #if \"O SR. PRESIDENTE (Jair Bolsonaro)\" in clean_string:\n",
    "     #   pattern = \"O SR\\. PRESIDENTE (\\(Jair Bolsonaro\\))?(.*?)((O?\\s?SR\\.?\\s+?)|(A?\\s?SRA\\.?\\s+?)|$)\"\n",
    "      #  group_no = 2\n",
    "\n",
    "    #else:\n",
    "    #pattern  = \"O?\\s?SR\\.?\\s?(DEPUTADO)?\\s+JAIR\\s+BOLSONARO\\s?(\\((Bloco\\/)?\\w{2,}\\s?\\-\\s?\\w{2}[^)]+\\))?(.*?)((O?\\s?SR\\.?\\s+?)|(A?\\s?SRA\\.?\\s+?)|$)\"\n",
    "    pattern = \"((O?\\s?SR\\.?\\s+?)|(A?\\s?SRA\\.?\\s+?))(\\s+DEPUTADO|\\s+DEPUTADA|\\s+PRESIDENTE|\\s+PRESIDENTA)?\"\n",
    "    group_no = 4\n",
    "        \n",
    "    regexp   = re.compile(pattern, re.MULTILINE)\n",
    "    matches  = re.finditer(regexp, clean_string)\n",
    "    \n",
    "    full_clean_quote    = [ ]\n",
    "    full_original_quote = [ ]\n",
    "    \n",
    "    for match in matches:\n",
    "                        \n",
    "        match_start = match.start(group_no)\n",
    "        match_end   = match.end(group_no)\n",
    "            \n",
    "        clean_quote = match[group_no]\n",
    "        clean_quote = clean_quote.replace(\"- \", \"\")\n",
    "        \n",
    "        original_quote = original_string[match_start:match_end]\n",
    "        original_quote = original_quote.replace(\"- \", \"\")\n",
    "        \n",
    "        full_clean_quote.append(clean_quote)\n",
    "        full_original_quote.append(original_quote)\n",
    "        \n",
    "    full_clean_quote    = ' [ INTERRUPÇÃO ] '.join(full_clean_quote)\n",
    "    full_original_quote = ' [ INTERRUPÇÃO ] '.join(full_original_quote)\n",
    "\n",
    "    # Remove espaços múltiplos internos usando a operação join de lista\n",
    "    full_clean_quote    = ' '.join(full_clean_quote.split()) \n",
    "    full_original_quote = ' '.join(full_original_quote.split()) \n",
    "    \n",
    "    return full_clean_quote, full_original_quote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções para aplicar operações de regex no dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_count_speakers(row):\n",
    "    \n",
    "    '''\n",
    "    Aplica, linha a linha, a função\n",
    "    find_speakers(string)\n",
    "    '''\n",
    "    \n",
    "    matches = find_speakers(row.CLEAN_CONTENT)\n",
    "    speaker_count = len(matches)\n",
    "    \n",
    "    return pd.Series({ \"SPEAKER_COUNT\":speaker_count \n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_extract_full_quote(row):\n",
    "    \n",
    "    '''\n",
    "    Aplica, linha a linha, a função\n",
    "    extract_full_quote(string)\n",
    "    '''\n",
    "    \n",
    "    full_clean_quote, full_original_quote = extract_full_quote(row.CLEAN_CONTENT, row.ORIGINAL_CONTENT)\n",
    "    return pd.Series({\n",
    "        \"FULL_CLEAN_QUOTE\"    : full_clean_quote,\n",
    "        \"FULL_ORIGINAL_QUOTE\" : full_original_quote\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função que encapsula anteriores e roda a operação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speak(zip_file = False):\n",
    "    '''\n",
    "    zip_file -> indica se os arquivos estão compactados no formato ZIP.\n",
    "    \n",
    "    Executa as operações anteriores em ambos os bancos de dados\n",
    "    (plenário e comissões), filtra entradas sem match, concatena\n",
    "    ambos os dataframes e salva para arquivo csv.\n",
    "    '''\n",
    "    list_info_file = []\n",
    "    list_speak = []\n",
    "    \n",
    "    if zip_file:\n",
    "        list_speak, list_info_file = read_file( find_fpaths(\"../data/txts/plenario/\", \"*.zip\"), zip_file)\n",
    "    else:\n",
    "        list_speak, list_info_file = read_file(find_fpaths(\"../data/txts/plenario/\", \"*.txt\"),zip_file)\n",
    "    \n",
    "    return list_speak, list_info_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Files...\n",
      "../data/txts/plenario\\2001.zip 19355\n",
      "../data/txts/plenario\\2002.zip 10353\n",
      "../data/txts/plenario\\2003.zip 23829\n",
      "../data/txts/plenario\\2004.zip 19028\n",
      "../data/txts/plenario\\2005.zip 20185\n",
      "../data/txts/plenario\\2006.zip 15522\n",
      "../data/txts/plenario\\2007.zip 25116\n",
      "../data/txts/plenario\\2008.zip 20488\n",
      "../data/txts/plenario\\2009.zip 24499\n",
      "../data/txts/plenario\\2010.zip 14961\n",
      "../data/txts/plenario\\2011.zip 22727\n",
      "../data/txts/plenario\\2012.zip 20031\n",
      "../data/txts/plenario\\2013.zip 27752\n",
      "../data/txts/plenario\\2014.zip 17303\n",
      "../data/txts/plenario\\2015.zip 28169\n",
      "../data/txts/plenario\\2016.zip 23092\n",
      "../data/txts/plenario\\2017.zip 28134\n",
      "../data/txts/plenario\\2018.zip 16560\n",
      "../data/txts/plenario\\2019.zip 19506\n"
     ]
    }
   ],
   "source": [
    "## LENDO OS DISCURSOS E SALVANDO EM UM DATAFRAME\n",
    "speak, info = get_speak(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speak[i],info[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making  DF plen...\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "df_plen = make_df_plen(speak,info)\n",
    "\n",
    "#As sessões do tipo HOMENAGEM são apenas registro de protocolo. \n",
    "#Não contém transcrição de discursos.\n",
    "df_plen = df_plen[ df_plen.SESSION_TYPE != \"HOMENAGEM\" ]\n",
    "\n",
    "# Aplica funções para extrair discursos\n",
    "df_plen[\"SPEAKER_COUNT\"] = df_plen.apply(apply_count_speakers, axis=1)\n",
    "\n",
    "#return df_plen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd de Registros por ANO:\n",
      "2015    7971\n",
      "2017    7923\n",
      "2013    7923\n",
      "2007    7264\n",
      "2009    7110\n",
      "2003    6726\n",
      "2016    6624\n",
      "2011    6464\n",
      "2008    5946\n",
      "2019    5777\n",
      "2012    5666\n",
      "2005    5620\n",
      "2001    5447\n",
      "2004    5442\n",
      "2014    4909\n",
      "2018    4701\n",
      "2006    4555\n",
      "2010    4247\n",
      "2002    2843\n",
      "Name: YEAR, dtype: int64\n",
      "\n",
      "Qtd Total de registros:113158 \n"
     ]
    }
   ],
   "source": [
    "print('Qtd de Registros por ANO:\\n{}\\n\\nQtd Total de registros:{} '.format(df_plen.YEAR.value_counts(),len(df_plen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva\n",
    "directory = \"../data/csvs/\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "name_file = directory + 'discursos_plen.csv'#str(df.YEAR.unique()[0]) + '_plen.csv'\n",
    "df_plen.to_csv(name_file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "session_date = []\n",
    "for file in info:\n",
    "    #print(i)\n",
    "    result = re.search(\"\\-([A-Z\\s]+)\\-\", file)# info[108251])\n",
    "    if result is None:\n",
    "        session_date.append('none')\n",
    "    else:\n",
    "        session_date.append(result.group(1))\n",
    "    #'SESSION_DATE'     : [ re.search(\"\\d{8}\", file).group(0) for file in content ]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118984"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(session_date)\n",
    "len(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
